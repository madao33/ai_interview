# å¤§æ¨¡å‹ç›¸å…³çŸ¥è¯†

è®°å½•å­¦ä¹ LLamaã€GPTã€ChatGLMçš„ç›¸å…³ç¬”è®°

ä¸»æµå¤§æ¨¡å‹ï¼š

- [ ] LLama
- [ ] GPT3
- [ ] ChatGPT
- [ ] ChatGLM
- [ ] T5

å‚è€ƒèµ„æ–™é“¾æ¥ï¼š

1. [å¤§æ¨¡å‹åŸºç¡€](https://github.com/datawhalechina/so-large-lm)
2. Attention is all you need
3. [Transformer(ä¸€)--è®ºæ–‡ç¿»è¯‘ï¼šAttention Is All You Need ä¸­æ–‡ç‰ˆ](https://blog.csdn.net/nocml/article/details/103082600)
4. [GPT **Improving Language Understanding by Generative Pre-Training**. 2018. ](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
5. [GPT-2 **Language Models are Unsupervised Multitask Learners**. 2018.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
6. [GPT-3 **"Language Models are Few-Shot Learners"**. NeurIPS 2020. ](https://arxiv.org/abs/2005.14165)
7. [InstructGPT: **Training language models to follow instructions with human feedback**, Arxiv 2022](https://arxiv.org/abs/2203.02155)
8. [GPT-4 **"GPT-4 Technical Report"**. 2023. ](https://arxiv.org/abs/2303.08774)

# åˆ†è¯

**åˆ†è¯å™¨**å°†ä»»æ„å­—ç¬¦ä¸²è½¬æ¢ä¸ºè¯å…ƒåºåˆ—ï¼š 'the mouse ate the cheese.' $\Rightarrow [the, mouse, ate, the, cheese, .]$ 

## åŸºäºç©ºæ ¼åˆ†è¯

åˆ†è¯ï¼Œå…¶å®ä»å­—é¢å¾ˆå¥½ç†è§£ï¼Œå°±æ˜¯æŠŠè¯åˆ†å¼€ï¼Œä»è€Œæ–¹ä¾¿å¯¹äºè¯è¿›è¡Œå•ç‹¬çš„ç¼–ç ï¼Œå¯¹äºè‹±æ–‡å­—æ¯æ¥è¯´ï¼Œç”±äºå…¶å¤©ç„¶çš„ä¸»è¦ç”±å•è¯+ç©ºæ ¼+æ ‡ç‚¹ç¬¦å·ç»„æˆï¼Œæœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨`text.split(' ')`æ–¹å¼è¿›è¡Œåˆ†è¯ï¼Œè¿™ç§åˆ†è¯æ–¹å¼å¯¹äºè‹±æ–‡è¿™ç§æŒ‰ç…§ç©ºæ ¼ï¼Œä¸”æ¯ä¸ªåˆ†è¯åçš„å•è¯æœ‰è¯­ä¹‰å…³ç³»çš„æ–‡æœ¬æ˜¯ç®€å•è€Œç›´æ¥çš„åˆ†è¯æ–¹å¼ã€‚ç„¶è€Œï¼Œå¯¹äºä¸€äº›è¯­è¨€ï¼Œå¦‚ä¸­æ–‡ï¼Œå¥å­ä¸­çš„å•è¯ä¹‹é—´æ²¡æœ‰ç©ºæ ¼ï¼Œä¾‹å¦‚ä¸‹æ–‡çš„å½¢å¼ã€‚

$$\text{"æˆ‘ä»Šå¤©å»äº†å•†åº—ã€‚"}$$

è¿˜æœ‰ä¸€äº›è¯­è¨€ï¼Œæ¯”å¦‚å¾·è¯­ï¼Œå­˜åœ¨ç€é•¿çš„å¤åˆè¯ï¼ˆä¾‹å¦‚`Abwasserbehandlungsanlange`ï¼‰ã€‚å³ä½¿åœ¨è‹±è¯­ä¸­ï¼Œä¹Ÿæœ‰è¿å­—ç¬¦è¯ï¼ˆä¾‹å¦‚father-in-lawï¼‰å’Œç¼©ç•¥è¯ï¼ˆä¾‹å¦‚don'tï¼‰ï¼Œå®ƒä»¬éœ€è¦è¢«æ­£ç¡®æ‹†åˆ†ã€‚ä¾‹å¦‚ï¼ŒPenn Treebankå°†don'tæ‹†åˆ†ä¸ºdoå’Œn'tï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è¯­è¨€ä¸ŠåŸºäºä¿¡æ¯çš„é€‰æ‹©ï¼Œä½†ä¸å¤ªæ˜æ˜¾ã€‚å› æ­¤ï¼Œä»…ä»…é€šè¿‡ç©ºæ ¼æ¥åˆ’åˆ†å•è¯ä¼šå¸¦æ¥å¾ˆå¤šé—®é¢˜ã€‚

é‚£ä¹ˆï¼Œä»€ä¹ˆæ ·çš„åˆ†è¯æ‰æ˜¯å¥½çš„å‘¢ï¼Ÿç›®å‰ä»ç›´è§‰å’Œå·¥ç¨‹å®è·µçš„è§’åº¦æ¥è¯´ï¼š

- é¦–å…ˆæˆ‘ä»¬ä¸å¸Œæœ›æœ‰å¤ªå¤šçš„è¯å…ƒï¼ˆæç«¯æƒ…å†µï¼šå­—ç¬¦æˆ–å­—èŠ‚ï¼‰ï¼Œå¦åˆ™åºåˆ—ä¼šå˜å¾—éš¾ä»¥å»ºæ¨¡ã€‚
- å…¶æ¬¡æˆ‘ä»¬ä¹Ÿä¸å¸Œæœ›è¯å…ƒè¿‡å°‘ï¼Œå¦åˆ™å•è¯ä¹‹é—´å°±æ— æ³•å…±äº«å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œmother-in-lawå’Œfather-in-lawåº”è¯¥å®Œå…¨ä¸åŒå—ï¼Ÿï¼‰ï¼Œè¿™å¯¹äºå½¢æ€ä¸°å¯Œçš„è¯­è¨€å°¤å…¶æ˜¯ä¸ªé—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œé˜¿æ‹‰ä¼¯è¯­ã€åœŸè€³å…¶è¯­ç­‰ï¼‰ã€‚
- æ¯ä¸ªè¯å…ƒåº”è¯¥æ˜¯ä¸€ä¸ªåœ¨è¯­è¨€æˆ–ç»Ÿè®¡ä¸Šæœ‰æ„ä¹‰çš„å•ä½ã€‚

## Unigram model (SentencePiece)

ä¸ä»…ä»…æ ¹æ®é¢‘ç‡è¿›è¡Œæ‹†åˆ†ä¸åŒï¼Œä¸€ä¸ªæ›´â€œæœ‰åŸåˆ™â€çš„æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ªç›®æ ‡å‡½æ•°æ¥æ•æ‰ä¸€ä¸ªå¥½çš„åˆ†è¯çš„ç‰¹å¾ï¼Œè¿™ç§åŸºäºç›®æ ‡å‡½æ•°çš„åˆ†è¯æ¨¡å‹å¯ä»¥é€‚åº”æ›´å¥½åˆ†è¯åœºæ™¯ï¼ŒUnigram modelå°±æ˜¯åŸºäºè¿™ç§åŠ¨æœºæå‡ºçš„ã€‚æˆ‘ä»¬ç°åœ¨æè¿°ä¸€ä¸‹unigramæ¨¡å‹ï¼ˆ[Kudoï¼Œ2018å¹´](https://arxiv.org/pdf/1804.10959.pdf)ï¼‰ã€‚

è¿™æ˜¯SentencePieceå·¥å…·ï¼ˆ[Kudoï¼†Richardsonï¼Œ2018å¹´](https://aclanthology.org/D18-2012.pdf)ï¼‰æ‰€æ”¯æŒçš„ä¸€ç§åˆ†è¯æ–¹æ³•ï¼Œä¸BPEä¸€èµ·ä½¿ç”¨ã€‚
å®ƒè¢«ç”¨æ¥è®­ç»ƒT5å’ŒGopheræ¨¡å‹ã€‚ç»™å®šä¸€ä¸ªåºåˆ— $x_{1:L}$ ï¼Œä¸€ä¸ªåˆ†è¯å™¨ $T$ æ˜¯ $p\left(x_{1: L}\right)=\prod_{(i, j) \in T} p\left(x_{i: j}\right)$ çš„ä¸€ä¸ªé›†åˆã€‚è¿™è¾¹ç»™å‡ºä¸€ä¸ªå®ä¾‹ï¼š

- è®­ç»ƒæ•°æ®ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼š $ğ–ºğ–»ğ–ºğ–»ğ–¼$
- åˆ†è¯ç»“æœ  $T={(1,2),(3,4),(5,5)}$ ï¼ˆå…¶ä¸­ $V=\{ğ–ºğ–»,ğ–¼\}$ ï¼‰
- ä¼¼ç„¶å€¼ï¼š $p(x_{1:L})=2/3â‹…2/3â‹…1/3=4/27$

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè®­ç»ƒæ•°æ®æ˜¯å­—ç¬¦ä¸²" $ğ–ºğ–»ğ–ºğ–»ğ–¼$ "ã€‚åˆ†è¯ç»“æœ  $T={(1,2),(3,4),(5,5)}$  è¡¨ç¤ºå°†å­—ç¬¦ä¸²æ‹†åˆ†æˆä¸‰ä¸ªå­åºåˆ—ï¼š $(ğ–º,ğ–»)ï¼Œ(ğ–º,ğ–»)ï¼Œ(ğ–¼)$ ã€‚è¯æ±‡è¡¨ $V=\{ğ–ºğ–»,ğ–¼\}$ è¡¨ç¤ºäº†è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„æ‰€æœ‰è¯æ±‡ã€‚

ä¼¼ç„¶å€¼  $p(x_{1:L})$ æ˜¯æ ¹æ® unigram æ¨¡å‹è®¡ç®—å¾—å‡ºçš„æ¦‚ç‡ï¼Œè¡¨ç¤ºè®­ç»ƒæ•°æ®çš„ä¼¼ç„¶åº¦ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¦‚ç‡çš„è®¡ç®—ä¸º  $2/3â‹…2/3â‹…1/3=4/27$ ã€‚è¿™ä¸ªå€¼ä»£è¡¨äº†æ ¹æ® unigram æ¨¡å‹ï¼Œå°†è®­ç»ƒæ•°æ®åˆ†è¯ä¸ºæ‰€ç»™çš„åˆ†è¯ç»“æœ $T $çš„æ¦‚ç‡ã€‚

unigram æ¨¡å‹é€šè¿‡ç»Ÿè®¡æ¯ä¸ªè¯æ±‡åœ¨è®­ç»ƒæ•°æ®ä¸­çš„å‡ºç°æ¬¡æ•°æ¥ä¼°è®¡å…¶æ¦‚ç‡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ $ğ–ºğ–»$ åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°äº†ä¸¤æ¬¡ï¼Œ $ğ–¼$ å‡ºç°äº†ä¸€æ¬¡ã€‚å› æ­¤ï¼Œæ ¹æ® unigram æ¨¡å‹çš„ä¼°è®¡ï¼Œ $p(ğ–ºğ–»)=2/3$ ï¼Œ $p(ğ–¼)=1/3$ ã€‚é€šè¿‡å°†å„ä¸ªè¯æ±‡çš„æ¦‚ç‡ç›¸ä¹˜ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ•´ä¸ªè®­ç»ƒæ•°æ®çš„ä¼¼ç„¶å€¼ä¸º $4/27$ ã€‚

ä¼¼ç„¶å€¼çš„è®¡ç®—æ˜¯ unigram æ¨¡å‹ä¸­é‡è¦çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒç”¨äºè¯„ä¼°åˆ†è¯ç»“æœçš„è´¨é‡ã€‚è¾ƒé«˜çš„ä¼¼ç„¶å€¼è¡¨ç¤ºè®­ç»ƒæ•°æ®ä¸åˆ†è¯ç»“æœä¹‹é—´çš„åŒ¹é…ç¨‹åº¦è¾ƒé«˜ï¼Œè¿™æ„å‘³ç€è¯¥åˆ†è¯ç»“æœè¾ƒä¸ºå‡†ç¡®æˆ–åˆç†ã€‚

# æ¨¡å‹æ¶æ„

## è¯­è¨€æ¨¡å‹åˆ†ç±»

å¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæœ€åˆçš„èµ·æºæ¥è‡ªäºTransformeræ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯ç¼–ç -è§£ç ç«¯ ï¼ˆEncoder-Decoderï¼‰çš„æ¶æ„ã€‚ä½†æ˜¯å½“å‰å¯¹äºè¯­è¨€æ¨¡å‹çš„åˆ†ç±»ï¼Œå°†è¯­è¨€æ¨¡å‹åˆ†ä¸ºä¸‰ä¸ªç±»å‹ï¼šç¼–ç ç«¯ï¼ˆEncoder-Onlyï¼‰ï¼Œè§£ç ç«¯ï¼ˆDecoder-Onlyï¼‰å’Œç¼–ç -è§£ç ç«¯ï¼ˆEncoder-Decoderï¼‰

### ç¼–ç ç«¯ï¼ˆEncoder-Onlyï¼‰æ¶æ„

ç¼–ç ç«¯æ¶æ„çš„è‘—åçš„æ¨¡å‹å¦‚BERTã€RoBERTaç­‰ã€‚è¿™äº›è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡è¡¨å¾ï¼Œä½†ä¸èƒ½ç›´æ¥ç”¨äºç”Ÿæˆæ–‡æœ¬ã€‚å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ $x_{1:L}â‡’Ï•(x_{1:L})$ ã€‚è¿™äº›ä¸Šä¸‹æ–‡å‘é‡è¡¨å¾é€šå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼ˆä¹Ÿè¢«ç§°ä¸ºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ï¼‰ã€‚ä»»åŠ¡å½¢å¼æ¯”è¾ƒç®€å•ï¼Œä¸‹é¢ä»¥æƒ…æ„Ÿåˆ†ç±»/è‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸¾ä¾‹ï¼š

$$
æƒ…æ„Ÿåˆ†æè¾“å…¥ä¸è¾“å‡ºå½¢å¼ï¼š[[CLS], ä»–ä»¬, ç§»åŠ¨, è€Œ, å¼ºå¤§]\Rightarrow æ­£é¢æƒ…ç»ª
$$

$$
è‡ªç„¶è¯­è¨€å¤„ç†è¾“å…¥ä¸è¾“å‡ºå½¢å¼ï¼š[[CLS], æ‰€æœ‰, åŠ¨ç‰©, éƒ½, å–œæ¬¢, åƒ, é¥¼å¹², å“¦]â‡’è•´æ¶µ
$$

è¯¥æ¶æ„çš„ä¼˜åŠ¿æ˜¯å¯¹äºæ–‡æœ¬çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æœ‰æ›´å¥½çš„ç†è§£ï¼Œå› æ­¤è¯¥æ¨¡å‹æ¶æ„æ‰ä¼šå¤šç”¨äºç†è§£ä»»åŠ¡ã€‚è¯¥æ¶æ„çš„ä¼˜ç‚¹æ˜¯å¯¹äºæ¯ä¸ª $x{i}$ ï¼Œä¸Šä¸‹æ–‡å‘é‡è¡¨å¾å¯ä»¥åŒå‘åœ°ä¾èµ–äºå·¦ä¾§ä¸Šä¸‹æ–‡ $(x_{1:iâˆ’1})$ å’Œå³ä¾§ä¸Šä¸‹æ–‡  $(x_{i+1:L})$ ã€‚ä½†æ˜¯ç¼ºç‚¹åœ¨äºä¸èƒ½è‡ªç„¶åœ°ç”Ÿæˆå®Œæˆæ–‡æœ¬ï¼Œä¸”éœ€è¦æ›´å¤šçš„ç‰¹å®šè®­ç»ƒç›®æ ‡ï¼ˆå¦‚æ©ç è¯­è¨€å»ºæ¨¡ï¼‰ã€‚

### è§£ç å™¨ï¼ˆDecoder-Onlyï¼‰æ¶æ„

è§£ç å™¨æ¶æ„çš„è‘—åæ¨¡å‹å°±æ˜¯å¤§åé¼é¼çš„GPTç³»åˆ—æ¨¡å‹ã€‚è¿™äº›æ˜¯æˆ‘ä»¬å¸¸è§çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œç»™å®šä¸€ä¸ªæç¤º 
 $x_{1:i}$ ï¼Œå®ƒä»¬å¯ä»¥ç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡è¡¨å¾ï¼Œå¹¶å¯¹ä¸‹ä¸€ä¸ªè¯å…ƒ $x_{i+1}$ ï¼ˆä»¥åŠé€’å½’åœ°ï¼Œæ•´ä¸ªå®Œæˆ 
 $x_{i+1:L}$ï¼‰ ç”Ÿæˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚ $x_{1:i}â‡’Ï•(x_{1:i}),p(x_{i+1}âˆ£x_{1:i})$ ã€‚æˆ‘ä»¬ä»¥è‡ªåŠ¨è¡¥å…¨ä»»åŠ¡æ¥è¯´ï¼Œè¾“å…¥ä¸è¾“å‡ºçš„å½¢å¼ä¸ºï¼Œ $[[CLS], ä»–ä»¬, ç§»åŠ¨, è€Œ]â‡’å¼ºå¤§$ ã€‚ä¸ç¼–ç ç«¯æ¶æ„æ¯”ï¼Œå…¶ä¼˜ç‚¹ä¸ºèƒ½å¤Ÿè‡ªç„¶åœ°ç”Ÿæˆå®Œæˆæ–‡æœ¬ï¼Œæœ‰ç®€å•çš„è®­ç»ƒç›®æ ‡ï¼ˆæœ€å¤§ä¼¼ç„¶ï¼‰ã€‚ç¼ºç‚¹ä¹Ÿå¾ˆæ˜æ˜¾ï¼Œå¯¹äºæ¯ä¸ª  $xi$ ï¼Œä¸Šä¸‹æ–‡å‘é‡è¡¨å¾åªèƒ½å•å‘åœ°ä¾èµ–äºå·¦ä¾§ä¸Šä¸‹æ–‡  ($x_{1:iâˆ’1}$) ã€‚

###  ç¼–ç -è§£ç ç«¯ï¼ˆEncoder-Decoderï¼‰æ¶æ„

ç¼–ç -è§£ç ç«¯æ¶æ„å°±æ˜¯æœ€åˆçš„Transformeræ¨¡å‹ï¼Œå…¶ä»–çš„è¿˜æœ‰å¦‚BARTã€T5ç­‰æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹åœ¨æŸç§ç¨‹åº¦ä¸Šç»“åˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ï¼šå®ƒä»¬å¯ä»¥ä½¿ç”¨åŒå‘ä¸Šä¸‹æ–‡å‘é‡è¡¨å¾æ¥å¤„ç†è¾“å…¥ $x_{1:L}$ ï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆè¾“å‡º $y_{1:L}$ ã€‚å¯ä»¥å…¬å¼åŒ–ä¸ºï¼š

$$
x1:Lâ‡’Ï•(x1:L),p(y1:Lâˆ£Ï•(x1:L))ã€‚
$$

ä»¥è¡¨æ ¼åˆ°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸ºä¾‹ï¼Œå…¶è¾“å…¥å’Œè¾“å‡ºçš„å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
[åç§°:, æ¤ç‰©, |, ç±»å‹:, èŠ±å‰, å•†åº—]â‡’[èŠ±å‰, æ˜¯, ä¸€, ä¸ª, å•†åº—]ã€‚
$$

è¯¥æ¨¡å‹çš„å…·æœ‰ç¼–ç ç«¯ï¼Œè§£ç ç«¯ä¸¤ä¸ªæ¶æ„çš„å…±åŒçš„ä¼˜ç‚¹ï¼Œå¯¹äºæ¯ä¸ª $x_{i}$ ï¼Œä¸Šä¸‹æ–‡å‘é‡è¡¨å¾å¯ä»¥åŒå‘åœ°ä¾èµ–äºå·¦ä¾§ä¸Šä¸‹æ–‡  $x_{1:iâˆ’1}$ ) å’Œå³ä¾§ä¸Šä¸‹æ–‡ ( $x_{i+1:L}$ )ï¼Œå¯ä»¥è‡ªç”±çš„ç”Ÿæˆæ–‡æœ¬æ•°æ®ã€‚ç¼ºç‚¹å°±è¯´éœ€è¦æ›´å¤šçš„ç‰¹å®šè®­ç»ƒç›®æ ‡ã€‚

# Transformerç»“æ„

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

![image-20241029171636763](assets/image-20241029171636763.png)

## ç¼–ç å’Œè§£ç ç»“æ„

**ç¼–ç å™¨**ï¼šç¼–ç å™¨ç”±6ä¸ªç›¸åŒçš„å±‚ç»„åˆè€Œæˆï¼Œæ¯ä¸€å±‚æœ‰ä¸¤ä¸ªå­å±‚ï¼šç¬¬ä¸€å±‚æ˜¯å¤šå¤´è‡ªæ³¨æ„æœºåˆ¶ï¼Œç¬¬äºŒå±‚æ˜¯ä¸€ä¸ªç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œã€‚è¿™ä¸¤ä¸ªå­å±‚éƒ½æ·»åŠ äº†ä¸€ä¸ªæ®‹å·®ç¥ç»ç½‘ç»œå’ŒLayerNormï¼Œæ‰€ä»¥è¿™ä¸¤ä¸ªå­å±‚çš„è¾“å‡ºä¸º $LayerNorm(x + Sublayer(x))$ï¼Œå…¶ä¸­ $Sublayer(x)$ æ˜¯å­å±‚çš„å®ç°ã€‚

**è§£ç å™¨**ï¼šè§£ç å™¨åŒæ ·ä¹Ÿæ˜¯6ä¸ªç›¸åŒçš„å±‚ç»„æˆã€‚è§£ç å™¨åŒ…å«ä¸¤ä¸ªå¤šå¤´æ³¨æ„æœºåˆ¶å’Œä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ¯ä¸€ä¸ªå­å±‚åŒæ ·åŒ…å«æ®‹å·®è¿æ¥å’ŒLayerNormã€‚ç¬¬ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚é‡‡ç”¨äº†æ©ç æ“ä½œï¼Œä¿è¯å¯¹iè¾“å…¥çš„é¢„æµ‹åªä¾èµ–äºå°äºiçš„è¾“å…¥ã€‚ç¬¬äºŒä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚çš„kã€vçŸ©é˜µä½¿ç”¨ç¼–ç å™¨çš„è¾“å‡ºç»“æœï¼ŒQæ¥è‡ªäºè§£ç å™¨çš„ç¬¬ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ã€‚

## Attention

![image-20241029174018580](assets/image-20241029174018580.png)

æ³¨æ„åŠ›å‡½æ•°å¯ä»¥æè¿°ä¸ºå°†æŸ¥è¯¢ï¼ˆqueryï¼‰å’Œä¸€ç»„é”®å€¼æ˜ å°„åˆ°è¾“å‡ºï¼ŒQueryï¼ˆQï¼‰ã€Keyï¼ˆKï¼‰å’Œ Valueï¼ˆVï¼‰éƒ½æ˜¯å‘é‡ã€‚

ä¸‰ä¸ªå‘é‡æ˜¯é€šè¿‡Embed Tokenå’Œä¸‰ä¸ªæƒé‡çŸ©é˜µ $W^Q, W^k, W^v$ ç›¸ä¹˜å¾—åˆ°ï¼Œ$ d_k$ è¡¨ç¤ºkeyå‘é‡çš„ç»´åº¦ï¼Œæœ€ç»ˆçš„Attentionçš„è®¡ç®—å¦‚ä¸‹ï¼š
$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

> $d_k$ è¶Šå¤§çš„è¯ï¼Œç‚¹ä¹˜çš„ä¹˜ç§¯ä¼šæ¯”è¾ƒå¤§ï¼Œå°†ç‚¹ä¹˜çš„ä¹˜ç§¯ä¹˜ä»¥ $\frac{1}{\sqrt{d_k}}$å¯ä»¥å‡å°‘æ•°å€¼èŒƒå›´ï¼Œè®©softmaxå‡½æ•°çš„æ¢¯åº¦æ›´å¹³æ»‘

```python
def attention(query, key, value, mask=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim = -1)
    return torch.matmul(p_attn, value)
```

åœ¨softmaxä¹‹å‰çš„maskæ“ä½œæ˜¯ç”¨ä¸€ä¸ªå¾ˆå°çš„å€¼å¯¹åˆ¶å®šä½ç½®è¿›è¡Œè¦†ç›–å¡«å……ï¼Œè¿™æ ·åœ¨ä¹‹åè®¡ç®—softmaxæ—¶ï¼Œç”±äºå¡«å……çš„å€¼å¾ˆå°ï¼Œæ‰€ä»¥è®¡ç®—å‡ºæ¥çš„æ¦‚ç‡ä¹Ÿä¼šå¾ˆå°ï¼ŒåŸºæœ¬å°±å¿½ç•¥äº†ã€‚

**LayerNormalizationæ¨¡å—**

ä¸è®ºæ˜¯layer normalizationè¿˜æ˜¯batch normalizationï¼Œå…¶å®åšçš„éƒ½æ˜¯ä¸€ä»¶äº‹æƒ…ï¼Œéƒ½æ˜¯æ ¹æ® $x = a*\frac{x-\overline{x}}{std+eps} + b $ å¯¹ x çš„åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ã€‚ä¸åŒçš„æ˜¯ $\overline{x}$ å’Œ $std$ çš„è®¡ç®—æ–¹å¼ä¸åŒ

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](assets/db1606b5c547b14692018d298cd4f9d3.png)

batch normalizationçš„ $\overline{x}$ å’Œ std æ˜¯å»¶ç²‰è‰²æ–¹å‘è®¡ç®—çš„ï¼Œè€Œlayer normalizationæ˜¯å»¶è“è‰²æ–¹å‘è®¡ç®—çš„ã€‚å¦‚æœå…„å¼Ÿä»¬å»é¢è¯•ï¼Œå¯èƒ½é¢è¯•å®˜ä¼šé—®ä¸ºä»€ä¹ˆè¿™é‡Œæ²¡æœ‰ä½¿ç”¨BNï¼Œè€Œä½¿ç”¨äº†LN,æˆ‘çš„ç†è§£æ˜¯ï¼ŒBNå¯¹batch sizeçš„å¤§å°æ˜¯æœ‰è¦æ±‚çš„ï¼Œä¸€èˆ¬batch sizeè¶Šå¤§ï¼Œè®¡ç®—å‡ºçš„ $\overline{x}$  è¶Šå¥½ï¼Œè€Œæˆ‘ç”¨12Gå†…å­˜çš„GPUï¼Œè·‘transformerçš„æ¨¡å‹æ—¶ï¼Œbatch sizeæœ€å¤šä¹Ÿå°±è®¾ç½®åˆ°32ã€‚batch sizeè¿˜æ˜¯åå°çš„ã€‚æ‰€ä»¥ä½¿ç”¨ä¸batch sizeæ— å…³çš„layer normlizationã€‚ä»å¦ä¸€ä¸ªè§’åº¦è®²ï¼Œ==batch sizeä¹‹æ‰€ä»¥å°ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬çš„embedding size å¤§ï¼Œè€Œlayer normalization æ­£å¥½æ˜¯å»¶è¿™ä¸ªæ–¹å‘åšçš„ï¼Œæ‰€ä»¥æ­£å¥½ä½¿å¾—layer normalizationè®¡ç®—çš„æ›´ç¨³å®š==ã€‚

## Multi-Head Attention

åˆ©ç”¨å­¦ä¹ åˆ°çš„ä¸åŒçš„çº¿æ€§æ˜ å°„å°†Q, K, Væ˜ å°„åˆ° $d_q, d_k, d_v$ ç»´åº¦ h æ¬¡ï¼Œåœ¨ä¸åŒçš„æ˜ å°„ Q, K, V å‘é‡ï¼Œå¹¶è¡Œçš„æ‰§è¡Œ Attention å‡½æ•°ï¼ŒConcatä¹‹åå†åšä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼Œå¾—åˆ°æœ€ç»ˆçš„å€¼ã€‚

Multi-head attentionå…è®¸æ¨¡å‹æŠŠä¸åŒä½ç½®å­åºåˆ—çš„è¡¨ç¤ºéƒ½æ•´åˆåˆ°ä¸€ä¸ªä¿¡æ¯ä¸­ã€‚å¦‚æœåªæœ‰ä¸€ä¸ªattention headï¼Œå®ƒçš„å¹³å‡å€¼ä¼šå‰Šå¼±è¿™ä¸ªä¿¡æ¯ã€‚
$$
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) & =\operatorname{Concat}\left(\operatorname{head}_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\
\text { where head } & =\operatorname{Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
$$

> å‰é¢æˆ‘ä»¬è¯´è¿‡äº†ï¼ŒQã€Kã€Vä¸‰ä¸ªçŸ©é˜µæ˜¯encoderçš„è¾“å…¥ç»è¿‡ä¸‰ä¸ªlinearæ˜ å°„è€Œæˆï¼Œå®ƒä»¬çš„å¤§å°æ˜¯[ B , L , D ] [B,L,D][B,L,D](batch size, max sentence length, embedding size), è¿™é‡Œä¸ºäº†è¯´çš„æ¸…æ¥šäº›ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸çœ‹Bè¿™ä¸ªç»´åº¦ã€‚é‚£ä¹ˆQã€Kã€Vçš„ç»´åº¦éƒ½ä¸º[ L , D ][L,D]ï¼Œmulti-headå°±æ˜¯åœ¨[ D ]ç»´åº¦ä¸Šå¯¹æ•°æ®è¿›è¡Œåˆ‡å‰²ï¼ŒæŠŠæ•°æ®åˆ‡æˆç­‰é•¿çš„8æ®µï¼ˆh = 8ï¼‰ï¼Œè¿™æ ·Qã€Kã€Vå‡è¢«åˆ‡æˆç­‰é•¿çš„8æ®µï¼Œç„¶åå¯¹åº”çš„Qã€Kã€Vå­æ®µç»„æˆä¸€ç»„ï¼Œæ¯ç»„é€šè¿‡ Scaled Dot-Product Attention ç®—æ³•è®¡ç®—å‡ºç»“æœï¼Œè¿™æ ·çš„ç»“æœæˆ‘ä»¬ä¼šå¾—åˆ°8ä¸ªï¼Œç„¶åæŠŠè¿™8ä¸ªç»“æœå†æ‹¼æˆä¸€ä¸ªç»“æœï¼Œå°±multi-headçš„ç»“æœã€‚å…·ä½“è¿‡ç¨‹å¦‚ä¸‹å›¾ï¼š
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](assets/c6669ff740a3d890341095fa2267fd78.png)

## Applications of Attention in our Model

multi-head attentionåœ¨Transformerä¸­æœ‰ä¸‰ç§ä¸åŒçš„ä½¿ç”¨æ–¹å¼ï¼š

- åœ¨encoder-decoder attentionå±‚ä¸­ï¼Œqueriesæ¥è‡ªå‰é¢çš„decoderå±‚ï¼Œè€Œkeyså’Œvaluesæ¥è‡ªencoderçš„è¾“å‡ºã€‚è¿™ä½¿å¾—decoderä¸­çš„æ¯ä¸ªä½ç½®éƒ½èƒ½å…³æ³¨åˆ°è¾“å…¥åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ã€‚ è¿™æ˜¯æ¨¡ä»¿åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸­å…¸å‹çš„ç¼–ç å™¨â€”è§£ç å™¨çš„attentionæœºåˆ¶ï¼Œä¾‹å¦‚ã€‚
- encoderåŒ…å«self-attentionå±‚ã€‚ åœ¨self-attentionå±‚ä¸­ï¼Œæ‰€æœ‰çš„keyã€valueå’Œqueryæ¥è‡ªåŒä¸€ä¸ªåœ°æ–¹ï¼Œåœ¨è¿™é‡Œæ˜¯encoderä¸­å‰ä¸€å±‚çš„è¾“å‡ºã€‚ encoderä¸­çš„æ¯ä¸ªä½ç½®éƒ½å¯ä»¥å…³æ³¨åˆ°encoderä¸Šä¸€å±‚çš„æ‰€æœ‰ä½ç½®ã€‚
- ç±»ä¼¼åœ°ï¼Œdecoderä¸­çš„self-attentionå±‚å…è®¸decoderä¸­çš„æ¯ä¸ªä½ç½®éƒ½å…³æ³¨decoderå±‚ä¸­å½“å‰ä½ç½®ä¹‹å‰çš„æ‰€æœ‰ä½ç½®ï¼ˆåŒ…æ‹¬å½“å‰ä½ç½®ï¼‰ã€‚==ä¸ºäº†ä¿æŒè§£ç å™¨çš„è‡ªå›å½’ç‰¹æ€§ï¼Œéœ€è¦é˜²æ­¢è§£ç å™¨ä¸­çš„ä¿¡æ¯å‘å·¦æµåŠ¨ã€‚æˆ‘ä»¬åœ¨scaled dot-product attentionçš„å†…éƒ¨ ï¼Œé€šè¿‡å±è”½softmaxè¾“å…¥ä¸­æ‰€æœ‰çš„éæ³•è¿æ¥å€¼ï¼ˆè®¾ç½®ä¸º âˆ’âˆï¼‰å®ç°äº†è¿™ä¸€ç‚¹==ã€‚

## Position-wise Feed-Forward Networks

é™¤äº†Attentionå­å±‚å¤–ï¼ŒEncoderå’ŒDecoderè¿˜åŒ…å«ä¸€ä¸ªå…¨é“¾æ¥çš„å‰é¦ˆç½‘ç»œï¼Œåˆ†åˆ«ä½œç”¨æ¯ä¸€ä¸ªä½ç½®ï¼Œå…¶åŒ…å«ä¸¤ä¸ªçº¿æ€§å˜æ¢ï¼Œä¸­é—´åŒ…å«ä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°
$$
FFN(x) = \max(0, xW_1+b_1)W_2+b_2
$$
çº¿æ€§å˜æ¢çš„å½¢å¼åœ¨ä¸åŒçš„ä½ç½®ç›¸åŒï¼Œä½†æ˜¯ä¸åŒçš„å±‚ä¹‹å‰ä½¿ç”¨çš„ä¸åŒçš„å‚æ•°ã€‚

## Embedding and Softmax

ä¸å…¶ä»–åºåˆ—è½¬æ¢æ¨¡å‹ç±»ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ åˆ°çš„åµŒå…¥è¯å‘é‡ å°†è¾“å…¥å­—ç¬¦å’Œè¾“å‡ºå­—ç¬¦è½¬æ¢ä¸ºç»´åº¦ä¸º $d_{model}$ çš„å‘é‡ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ™®é€šçš„çº¿æ€§å˜æ¢å’Œsoftmaxå‡½æ•°å°†decoderè¾“å‡ºè½¬æ¢ä¸ºé¢„æµ‹çš„ä¸‹ä¸€ä¸ªè¯ç¬¦çš„æ¦‚ç‡ã€‚ä¸¤ä¸ªåµŒå…¥å±‚ä¹‹é—´å’Œpre-softmaxçº¿æ€§å˜æ¢å…±äº«ç›¸åŒçš„æƒé‡çŸ©é˜µã€‚ åœ¨åµŒå…¥å±‚ä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›æƒé‡ä¹˜ä»¥ $\sqrt{d_{model}}$

## Positional Encoding

ç”±äºæˆ‘ä»¬çš„æ¨¡å‹ä¸åŒ…å«å¾ªç¯æˆ–å·ç§¯ï¼Œä¸ºäº†è®©æ¨¡å‹åˆ©ç”¨åºåˆ—çš„é¡ºåºä¿¡æ¯ï¼Œæˆ‘ä»¬å¿…é¡»åŠ å…¥åºåˆ—ä¸­å…³äºå­—ç¬¦ç›¸å¯¹æˆ–è€…ç»å¯¹ä½ç½®çš„ä¸€äº›ä¿¡æ¯ã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨encoderå’Œdecoderå †æ ˆåº•éƒ¨çš„è¾“å…¥åµŒå…¥ä¸­æ·»åŠ â€œä½ç½®ç¼–ç â€ã€‚ ä½ç½®ç¼–ç å’ŒåµŒå…¥çš„ç»´åº¦ $d_{model}$ ç›¸åŒï¼Œæ‰€ä»¥å®ƒä»¬ä¸¤ä¸ªå¯ä»¥ç›¸åŠ ã€‚æœ‰å¤šç§ä½ç½®ç¼–ç å¯ä»¥é€‰æ‹©ï¼Œä¾‹å¦‚é€šè¿‡å­¦ä¹ å¾—åˆ°çš„ä½ç½®ç¼–ç å’Œå›ºå®šçš„ä½ç½®ç¼–ç 

ä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼š
$$
\begin{aligned}
P E_{(p o s, 2 i)} & =\sin \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right) \\
P E_{(p o s, 2 i+1)} & =\cos \left(p o s / 10000^{2 i / d_{\mathrm{model}}}\right)
\end{aligned}
$$
å…¶ä¸­pos æ˜¯ä½ç½®ï¼Œi æ˜¯ç»´åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ç½®ç¼–ç çš„æ¯ä¸ªç»´åº¦å¯¹åº”äºä¸€ä¸ªæ­£å¼¦æ›²çº¿ã€‚æ³¢é•¿å½¢æˆäº†ä»2Ï€åˆ°10000Â·2Ï€çš„å‡ ä½•æ•°åˆ—ã€‚æˆ‘ä»¬ä¹‹æ‰€ä»¥é€‰æ‹©è¿™ä¸ªå‡½æ•°ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬å‡è®¾å®ƒå¯ä»¥è®©æ¨¡å‹å¾ˆå®¹æ˜“åœ°é€šè¿‡ç›¸å¯¹ä½ç½®æ¥å­¦ä¹ ,å› ä¸ºå¯¹ä»»æ„ç¡®å®šçš„åç§» $k,PE_{pos+k}$ å¯ä»¥è¡¨ç¤ºä¸º $PE_{pos}$  çš„çº¿æ€§å‡½æ•°ã€‚

>  æ¯ä¸ªä½ç½®ï¼ˆposï¼‰çš„PEå€¼å‡ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ ¹æ®PEçš„å€¼åŒºåˆ†ä½ç½®ï¼Œ**è€Œç”±ä¸Šé¢çš„çº¿æ€§å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥è®¡é‡å‡ºä¸¤ä¸ªä½ç½®çš„ç›¸å¯¹è·ç¦»**ã€‚

# GPT

[GPT **Improving Language Understanding by Generative Pre-Training**. 2018. ](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) æ˜¯OpenAIç ”ç©¶å›¢é˜Ÿå‘å¸ƒçš„ä¸€ç¯‡è®ºæ–‡ã€‚





# BERT







# GPT2







# GPT-3







# LLaMAæ¨¡å‹

























